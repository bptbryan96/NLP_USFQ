{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c88e42-50d4-4b4d-bdf1-894bcff1a41d",
   "metadata": {},
   "source": [
    "# NLP and Neural Networks\n",
    "\n",
    "In this exercise, we'll apply our knowledge of neural networks to process natural language. As we did in the bigram exercise, the goal of this lab is to predict the next word, given the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132d376-54d7-48c3-a52c-ac3d94ed798b",
   "metadata": {},
   "source": [
    "### Data set\n",
    "\n",
    "Load the text from \"One Hundred Years of Solitude\" that we used in our bigrams exercise. It's located in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e309d79-7746-40e3-8a02-3cc7b45c16ac",
   "metadata": {},
   "source": [
    "### Important note:\n",
    "\n",
    "Start with a smaller part of the text. Maybe the first 10 parragraphs, as the number of tokens rapidly increases as we add more text. \n",
    "\n",
    "Later you can use a bigger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bbced32-a252-48b0-bc8f-cecfdcf1ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Paso 1: Cargar el archivo de texto\n",
    "file_path = 'data\\cap1.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Paso 2: Preprocesar el texto\n",
    "text = text.lower() # Convertir el texto a minúsculas\n",
    "text = re.sub(r'\\s+', ' ', text)  # Eliminar espacios extra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1393d6-7cbf-4e8e-a699-0f0cd28982a3",
   "metadata": {},
   "source": [
    "Don't forget to prepare the data by generating the corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c33da77-ad0b-4eeb-9eaa-0dc98485187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['muchos', 'años', 'después', ',', 'frente', 'al', 'pelotón', 'de', 'fusilamiento', ',']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenizar el texto\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Mostrar los primeros 10 tokens\n",
    "print(tokens[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681843a-18f0-4d7c-9b02-83015f4383e1",
   "metadata": {},
   "source": [
    "### Let's prepare the data set.\n",
    "\n",
    "Our neural network needs to have an input X and an output y. Remember that these sets are numerical, so you'd need something to map the tokens into numbers, and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c820ccde-c2ee-41ef-b840-41ccca58b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('muchos', 'años'), 3), (('años', 'después'), 2), (('después', ','), 2), ((',', 'frente'), 1), (('frente', 'al'), 1), (('al', 'pelotón'), 2), (('pelotón', 'de'), 2), (('de', 'fusilamiento'), 2), (('fusilamiento', ','), 2), ((',', 'el'), 10)]\n"
     ]
    }
   ],
   "source": [
    "# in this case, let's consider a bigram (w1, w2)\n",
    "# assign the w1 to the X vector, and w2 to the y vector, why do we do this?\n",
    "'''Asignamos w1 a X y w2 a y, para entrenar el modelo y prediga la próxima palabra \n",
    "dada la palabra actual, capturando así las relaciones directas entre palabras'''\n",
    "\n",
    "# Crear diccionario de bigramas\n",
    "bigrams = {}\n",
    "for w1, w2 in zip(tokens, tokens[1:]):\n",
    "    bigram = (w1, w2)\n",
    "    bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "\n",
    "# Mostrar algunos bigramas y sus conteos\n",
    "print(list(bigrams.items())[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29c10640-f146-478a-a1a4-d2e747af5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget that since we are using torch, our training set vectors should be tensors\n",
    "# Crear el vocabulario a partir de tokens\n",
    "word_to_idx = {word: idx + 1 for idx, word in enumerate(set(tokens))}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Preparar las secuencias de entrada y salida\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "for (w1, w2), count in bigrams.items():\n",
    "    input_sequences.append(word_to_idx[w1])\n",
    "    output_sequences.append(word_to_idx[w2])\n",
    "\n",
    "# Convertir listas a tensores\n",
    "input_tensor = torch.tensor(input_sequences, dtype=torch.long)\n",
    "output_tensor = torch.tensor(output_sequences, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5984fd00-bdbf-4403-a341-b7ef83138db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our vectors are integers, which can be thought as a categorical variables.\n",
    "# torch provides the one_hot method, that would generate tensors suitable for our nn\n",
    "# make sure that the dtype of your tensor is float.\n",
    "\n",
    "# Convertir entradas a codificación one-hot\n",
    "vocab_size = len(word_to_idx) + 1  # Tamaño del vocabulario\n",
    "input_one_hot = F.one_hot(input_tensor, num_classes=vocab_size).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda25114-c6ae-4e07-a743-12e10cd77796",
   "metadata": {},
   "source": [
    "### Network design\n",
    "To start, we are going to have a very simple network. Define a single layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82bfac7a-e670-4aaf-bf30-5aa8d0ca46e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleBigramModel(\n",
      "  (fc): Linear(in_features=2033, out_features=2033, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# How many neurons should our input layer have?\n",
    "# Use as many neurons as the total number of categories (from your one-hot encoded tensors)\n",
    "\n",
    "# Definir el modelo de red neuronal simple\n",
    "class SimpleBigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SimpleBigramModel, self).__init__()\n",
    "        self.fc = nn.Linear(vocab_size, vocab_size)  # Capa lineal\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return torch.softmax(out, dim=1)  # Activación softmax para convertir en probabilidades\n",
    "\n",
    "# Inicializar el modelo, la función de pérdida y el optimizador\n",
    "model = SimpleBigramModel(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Mostrar la estructura de la red\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15600485-24e3-4716-92ed-28ac1aa792bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [10/50], Pérdida: 7.6172\n",
      "Época [20/50], Pérdida: 7.6170\n",
      "Época [30/50], Pérdida: 7.6169\n",
      "Época [40/50], Pérdida: 7.6166\n",
      "Época [50/50], Pérdida: 7.6162\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Train your network\n",
    "\n",
    "# Número de épocas para entrenamiento\n",
    "num_epochs = 50\n",
    "\n",
    "# Ciclo de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    # Resetear gradientes\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_one_hot)\n",
    "    \n",
    "    # Calcular la pérdida\n",
    "    loss = criterion(outputs, output_tensor)\n",
    "    \n",
    "    # Backward pass y optimización\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Mostrar la pérdida cada 10 épocas\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Época [{epoch + 1}/{num_epochs}], Pérdida: {loss.item():.4f}')\n",
    "\n",
    "print(\"Entrenamiento completado.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d09aa-8a47-4668-b1b1-5080be8851ed",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06d9c5-4df1-4145-812f-ff86958154c1",
   "metadata": {},
   "source": [
    "1. Test your network with a few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aab9f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra de prueba: macondo\n",
      "Predicciones (Top 5):\n",
      "  se: 0.0024\n",
      "  a: 0.0024\n",
      "  ,: 0.0024\n",
      "  en: 0.0024\n",
      "  para: 0.0024\n",
      "Verosimilitud negativa para 'macondo': 7.6165\n",
      "\n",
      "Palabra de prueba: soledad\n",
      "Predicciones (Top 5):\n",
      "  y: 0.0023\n",
      "  un: 0.0014\n",
      "  ,: 0.0014\n",
      "  hasta: 0.0014\n",
      "  le: 0.0014\n",
      "Verosimilitud negativa para 'soledad': 7.6173\n",
      "\n",
      "Palabra de prueba: años\n",
      "Predicciones (Top 5):\n",
      "  y: 0.0024\n",
      "  en: 0.0024\n",
      "  ,: 0.0024\n",
      "  más: 0.0023\n",
      "  .: 0.0022\n",
      "Verosimilitud negativa para 'años': 7.6165\n",
      "\n",
      "Palabra de prueba: los\n",
      "Predicciones (Top 5):\n",
      "  años: 0.0026\n",
      "  más: 0.0026\n",
      "  gitanos: 0.0025\n",
      "  hombres: 0.0024\n",
      "  niños: 0.0023\n",
      "Verosimilitud negativa para 'los': 7.6166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Seleccionar algunas palabras para probar\n",
    "test_words = ['macondo', 'soledad', 'años', 'los']\n",
    "\n",
    "# Convertir palabras de prueba a índices y luego a codificación one-hot\n",
    "test_indices = [word_to_idx[word] for word in test_words]\n",
    "test_tensor = torch.tensor(test_indices, dtype=torch.long)\n",
    "test_one_hot = F.one_hot(test_tensor, num_classes=vocab_size).float()\n",
    "\n",
    "# Obtener las predicciones de la red\n",
    "outputs = model(test_one_hot)\n",
    "\n",
    "# Definir la función de pérdida para calcular la verosimilitud negativa\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mostrar las predicciones y calcular la verosimilitud negativa\n",
    "for i, word in enumerate(test_words):\n",
    "    print(f\"Palabra de prueba: {word}\")\n",
    "    print(f\"Predicciones (Top 5):\")\n",
    "    top5_prob, top5_idx = torch.topk(outputs[i], 5)\n",
    "    \n",
    "    for j in range(5):\n",
    "        predicted_word = idx_to_word[top5_idx[j].item()]\n",
    "        print(f\"  {predicted_word}: {top5_prob[j].item():.4f}\")\n",
    "    \n",
    "    # Calcular la verosimilitud negativa para la palabra\n",
    "    target_idx = torch.tensor([test_indices[i]], dtype=torch.long)\n",
    "    negative_likelihood = criterion(outputs[i].unsqueeze(0), target_idx)\n",
    "    \n",
    "    print(f\"Verosimilitud negativa para '{word}': {negative_likelihood.item():.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e1d19b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macondo se lo que lo que\n",
      "soledad y las de melquíades se\n",
      "los años y las de melquíades\n",
      "Verosimilitud negativa para la oración 'macondo se lo que lo que': 7.6153\n",
      "Verosimilitud negativa para la oración 'soledad y las de melquíades se': 7.6152\n",
      "Verosimilitud negativa para la oración 'los años y las de melquíades': 7.6152\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir una función para generar una frase con el modelo simple\n",
    "def generate_sentence(start_word, model, length=5):\n",
    "    current_word = start_word\n",
    "    sentence = [current_word]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        idx = word_to_idx.get(current_word, 0)  # Obtener el índice de la palabra actual\n",
    "        input_tensor = torch.tensor([idx], dtype=torch.long)\n",
    "        input_one_hot = F.one_hot(input_tensor, num_classes=vocab_size).float()\n",
    "        \n",
    "        output = model(input_one_hot)  # Obtener la predicción del modelo\n",
    "        top_word_idx = torch.argmax(output, dim=1).item()  # Índice de la palabra más probable\n",
    "        current_word = idx_to_word.get(top_word_idx, '<UNK>')  # Obtener la palabra correspondiente\n",
    "        \n",
    "        sentence.append(current_word)  # Añadir la palabra a la oración\n",
    "    \n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Función para calcular la verosimilitud negativa de una frase\n",
    "def calculate_negative_likelihood(sentence, model):\n",
    "    test_indices = [word_to_idx[word] for word in sentence]\n",
    "    test_tensor = torch.tensor(test_indices[:-1], dtype=torch.long)  # Excluir la última palabra\n",
    "    test_target = torch.tensor(test_indices[1:], dtype=torch.long)   # Excluir la primera palabra\n",
    "    \n",
    "    test_one_hot = F.one_hot(test_tensor, num_classes=vocab_size).float()\n",
    "    outputs = model(test_one_hot)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs, test_target)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Generar frases con el modelo simple\n",
    "print(generate_sentence('macondo', model))\n",
    "print(generate_sentence('soledad', model))\n",
    "print(generate_sentence('los', model))\n",
    "\n",
    "# Calcular la verosimilitud negativa de las frases generadas\n",
    "generated_sentence_1 = generate_sentence('macondo', model).split()\n",
    "generated_sentence_2 = generate_sentence('soledad', model).split()\n",
    "generated_sentence_3 = generate_sentence('los', model).split()\n",
    "\n",
    "print(f\"Verosimilitud negativa para la oración '{' '.join(generated_sentence_1)}': {calculate_negative_likelihood(generated_sentence_1, model):.4f}\")\n",
    "print(f\"Verosimilitud negativa para la oración '{' '.join(generated_sentence_2)}': {calculate_negative_likelihood(generated_sentence_2, model):.4f}\")\n",
    "print(f\"Verosimilitud negativa para la oración '{' '.join(generated_sentence_3)}': {calculate_negative_likelihood(generated_sentence_3, model):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e838c-3cab-4b6f-b041-1fde6a6d29aa",
   "metadata": {},
   "source": [
    "**2. What does each value in the tensor represents?**\n",
    "\n",
    "Cada valor en el tensor de salida representa la probabilidad predicha de que una palabra específica en el vocabulario sea la siguiente palabra, dado el contexto de entrada. Estos valores son generados por la función softmax, que convierte los puntajes en probabilidades que suman a 1.\n",
    "\n",
    "**3. Why does it make sense to choose that number of neurons in our layer?**\n",
    "\n",
    "Tiene sentido elegir el número de neuronas en la capa de entrada igual al tamaño del vocabulario porque cada neurona corresponde a una palabra única en el conjunto de datos. Esto permite que el modelo aprenda una representación de cada palabra y pueda hacer predicciones precisas sobre la siguiente palabra en base a la entrada.\n",
    "\n",
    "**4. What's the negative likelihood for each example?**\n",
    "\n",
    "Para cada palabra generada el valor esta por 7.61, lo que se considera alto\n",
    "\n",
    "**5. Try generating a few sentences?**\n",
    "\n",
    "Se ha realizado\n",
    "\n",
    "**6. What's the negative likelihood for each sentence?**\n",
    "\n",
    "Para cada frase generada el valor esta por 7.61, lo que se considera alto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e533cf-33b0-4a61-84cb-57c5793893ee",
   "metadata": {},
   "source": [
    "### Design your own neural network (more layers and different number of neurons)\n",
    "The goal is to get sentences that make more sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1f106c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [10/25], Pérdida: 7.6173\n",
      "Época [20/25], Pérdida: 7.6173\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo de red neuronal con capas más profundas\n",
    "class DeepBigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=8, dropout_rate=0.5):\n",
    "        super(DeepBigramModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Inicializar el modelo, la función de pérdida y el optimizador\n",
    "model = DeepBigramModel(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Número de épocas para entrenamiento\n",
    "num_epochs = 25\n",
    "\n",
    "# Ciclo de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    # Resetear gradientes\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_one_hot)\n",
    "    \n",
    "    # Calcular la pérdida\n",
    "    loss = criterion(outputs, output_tensor)\n",
    "    \n",
    "    # Backward pass y optimización\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Mostrar la pérdida cada 10 épocas\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Época [{epoch + 1}/{num_epochs}], Pérdida: {loss.item():.4f}')\n",
    "\n",
    "print(\"Entrenamiento completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8ca9cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra de prueba: macondo\n",
      "Predicciones (Top 5):\n",
      "  adiestrarlos: 0.0008\n",
      "  abejas: 0.0008\n",
      "  podrá: 0.0008\n",
      "  al: 0.0008\n",
      "  arrastrado: 0.0008\n",
      "Verosimilitud negativa para 'macondo': 7.6171\n",
      "\n",
      "Palabra de prueba: soledad\n",
      "Predicciones (Top 5):\n",
      "  seguir: 0.0007\n",
      "  internas: 0.0007\n",
      "  descendencia: 0.0007\n",
      "  avanzaron: 0.0007\n",
      "  brújula: 0.0007\n",
      "Verosimilitud negativa para 'soledad': 7.6173\n",
      "\n",
      "Palabra de prueba: años\n",
      "Predicciones (Top 5):\n",
      "  adiestrarlos: 0.0008\n",
      "  abejas: 0.0008\n",
      "  podrá: 0.0008\n",
      "  arrastrado: 0.0008\n",
      "  espera: 0.0008\n",
      "Verosimilitud negativa para 'años': 7.6171\n",
      "\n",
      "Palabra de prueba: los\n",
      "Predicciones (Top 5):\n",
      "  seguir: 0.0007\n",
      "  internas: 0.0007\n",
      "  descendencia: 0.0007\n",
      "  brújula: 0.0007\n",
      "  poco: 0.0007\n",
      "Verosimilitud negativa para 'los': 7.6171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Seleccionar algunas palabras para probar\n",
    "test_words = ['macondo', 'soledad', 'años', 'los']\n",
    "\n",
    "# Convertir palabras de prueba a índices y luego a codificación one-hot\n",
    "test_indices = [word_to_idx[word] for word in test_words]\n",
    "test_tensor = torch.tensor(test_indices, dtype=torch.long)\n",
    "test_one_hot = F.one_hot(test_tensor, num_classes=vocab_size).float()\n",
    "\n",
    "# Obtener las predicciones de la red\n",
    "outputs = model(test_one_hot)\n",
    "\n",
    "# Definir la función de pérdida para calcular la verosimilitud negativa\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mostrar las predicciones y calcular la verosimilitud negativa\n",
    "for i, word in enumerate(test_words):\n",
    "    print(f\"Palabra de prueba: {word}\")\n",
    "    print(f\"Predicciones (Top 5):\")\n",
    "    top5_prob, top5_idx = torch.topk(outputs[i], 5)\n",
    "    \n",
    "    for j in range(5):\n",
    "        predicted_word = idx_to_word[top5_idx[j].item()]\n",
    "        print(f\"  {predicted_word}: {top5_prob[j].item():.4f}\")\n",
    "    \n",
    "    # Calcular la verosimilitud negativa para la palabra\n",
    "    target_idx = torch.tensor([test_indices[i]], dtype=torch.long)\n",
    "    negative_likelihood = criterion(outputs[i].unsqueeze(0), target_idx)\n",
    "    \n",
    "    print(f\"Verosimilitud negativa para '{word}': {negative_likelihood.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e639d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macondo magisterio adiestrarlos permaneció condujo abejas\n",
      "soledad seguir seguir seguir seguir adiestrarlos\n",
      "los magisterio adiestrarlos arrastrado magisterio seguir\n",
      "Verosimilitud negativa para la oración 'macondo adiestrarlos abejas adiestrarlos permaneció permaneció': 7.6170\n",
      "Verosimilitud negativa para la oración 'soledad macondo seguir adiestrarlos adiestrarlos abejas': 7.6171\n",
      "Verosimilitud negativa para la oración 'los adiestrarlos seguir seguir permaneció permaneció': 7.6170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir una función para generar una frase con el modelo simple\n",
    "def generate_sentence(start_word, model, length=5):\n",
    "    current_word = start_word\n",
    "    sentence = [current_word]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        idx = word_to_idx.get(current_word, 0)  # Obtener el índice de la palabra actual\n",
    "        input_tensor = torch.tensor([idx], dtype=torch.long)\n",
    "        input_one_hot = F.one_hot(input_tensor, num_classes=vocab_size).float()\n",
    "        \n",
    "        output = model(input_one_hot)  # Obtener la predicción del modelo\n",
    "        top_word_idx = torch.argmax(output, dim=1).item()  # Índice de la palabra más probable\n",
    "        current_word = idx_to_word.get(top_word_idx, '<UNK>')  # Obtener la palabra correspondiente\n",
    "        \n",
    "        sentence.append(current_word)  # Añadir la palabra a la oración\n",
    "    \n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Función para calcular la verosimilitud negativa de una frase\n",
    "def calculate_negative_likelihood(sentence, model):\n",
    "    test_indices = [word_to_idx[word] for word in sentence]\n",
    "    test_tensor = torch.tensor(test_indices[:-1], dtype=torch.long)  # Excluir la última palabra\n",
    "    test_target = torch.tensor(test_indices[1:], dtype=torch.long)   # Excluir la primera palabra\n",
    "    \n",
    "    test_one_hot = F.one_hot(test_tensor, num_classes=vocab_size).float()\n",
    "    outputs = model(test_one_hot)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs, test_target)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Generar frases con el modelo simple\n",
    "print(generate_sentence('macondo', model))\n",
    "print(generate_sentence('soledad', model))\n",
    "print(generate_sentence('los', model))\n",
    "\n",
    "# Calcular la verosimilitud negativa de las frases generadas\n",
    "generated_sentence_1 = generate_sentence('macondo', model).split()\n",
    "generated_sentence_2 = generate_sentence('soledad', model).split()\n",
    "generated_sentence_3 = generate_sentence('los', model).split()\n",
    "\n",
    "print(f\"Verosimilitud negativa para la oración '{' '.join(generated_sentence_1)}': {calculate_negative_likelihood(generated_sentence_1, model):.4f}\")\n",
    "print(f\"Verosimilitud negativa para la oración '{' '.join(generated_sentence_2)}': {calculate_negative_likelihood(generated_sentence_2, model):.4f}\")\n",
    "print(f\"Verosimilitud negativa para la oración '{' '.join(generated_sentence_3)}': {calculate_negative_likelihood(generated_sentence_3, model):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9594c7",
   "metadata": {},
   "source": [
    "## **Conclusión**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac5a71",
   "metadata": {},
   "source": [
    "Después de realizar múltiples pruebas variando la cantidad de capas, el número de épocas y la tasa de aprendizaje, no se observó una mejora significativa en las frases generadas en comparación con el modelo más simple. Esto sugiere que podrían ser necesarias otras técnicas de procesamiento de datos o diferentes tipos de modelos para mejorar el rendimiento. Además, podría ser crucial contar con un conjunto de datos mucho más grande y desarrollar un manejo más detallado de las relaciones entre palabras y frases para obtener resultados más efectivos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
